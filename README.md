# Jelly Belly API Data Collection and Processing Workflow

## Overview

This document outlines the workflow used for scraping, processing, and preparing Jelly Belly flavor data for API integration.

## 'Bean' class scraping process:

### 1. Data Collection

#### Static Data Scraping (`staticScraper.py`)

- **Purpose**: This script is designed to scrape static data from the Jelly Belly flavor collections webpage. It specifically collects details about different Jelly Belly flavors, including their group names, individual flavor names, background colors, and associated image URLs.
- **Input**: The script uses the URL 'https://www.jellybelly.com/jelly-belly-flavor-collections' to scrape data.
- **Output**: Data exported to `static_data.json`.

#### Dynamic Data Scraping (`dynamicScraper.py` & `getXpathsBtns.py`)

- **Purpose**:

  - `getXpathsBtns.py`: This script is designed to generate XPath expressions for target elements, specifically 'Learn More' buttons, on the Jelly Belly single flavors webpage. It uses these XPath expressions to facilitate the dynamic scraping of flavor data.

    - **Input**: The script navigates to 'https://www.jellybelly.com/jelly-belly-single-flavors' to find the target elements.
    - **Output**: `get_xpaths_btn.json` - Contains the XPath expressions for the buttons on the webpage.

  - `dynamicScraper.py`: This script scrapes dynamic data from the Jelly Belly single flavors webpage using the XPaths generated by `getXpathsBtns.py`. It collects details such as flavor name, description, and ingredients.
    - **Input**: The script uses the XPaths from `get_xpaths_btn.json` to click on buttons and scrape dynamic content from the same Jelly Belly webpage.
    - **Output**: dynamic_data.json - The scraped dynamic data, including flavor name, description, and ingredients, is saved in this JSON file.

### 2. Data Merging and Cleaning

#### Merging Data (`mergeBeansData.py`)

- **Purpose**:
  - This script is designed to load, normalize, and merge data from `static_data.json` and `dynamic_data.json`. It ensures the data is consistent by normalizing the FlavorName field and merges the datasets based on this field. The script also keeps track of and exports any unmatched items from both datasets.
- **Process**:
  - **Normalization**: Flavor names from both datasets are normalized for consistency.
  - **Merging**: Merges the normalized data based on the FlavorName field.
  - **Tracking Unmatched Data**: Identifies and separates items that do not find a match in either dataset.
  - **Additional Fields**: Adds default values for fields like 'GlutenFree', 'SugarFree', 'Seasonal', and 'Kosher' in the merged data.
- **Input**:
  - `static_data.json` and `dynamic_data.json` - The datasets to be merged.
- **Outputs**:
  - `merged_beans_data.json` - Contains the merged data with additional fields.
  - `unmatched_static_data.json` - Contains static data items that didn't find a match.
  - `unmatched_dynamic_data.json` - Contains dynamic data items that didn't find a match.

### 3. Data Validation

#### Searching for Missing Names (`searchForMissingNames.py`)

note : `beans_name_list.json` - is a copy of `get_xpaths_btn.json` with text modification - isolate bean's name from it's xpath.

- **Purpose**: Compares expected flavor list (`beans_name_list.json`) with actual entries in `merged_beans_data.json` (to make sure no information was omitted).
- Identifies and displays missing flavors in terminal.
- **Input**: `beans_name_list.json` and `merged_beans_data.json`.
- **Outputs**: list of 'missing' data identified by flavorName to the terminal.

### 4. Data Formatting

#### Capitalization (`toCapitalize.py`)

- **Purpose**: Capitalizes strings in the data, keys to capitalize = "GroupName", "FlavorName", "Description", "Ingredients".
- **Input**: Processed data in `merged_beans_data.json` (after checking no beans are missing).
- **Output**: `toCapitalize.json`.

#### Preparation for API Seeding (`prepDataToSeedWithColor.py`)

- **Purpose**:
  - Formats data for seeding into the API.
  - Add color information to each bean - the script finds the closest CSS3 color name to a given hex color `backgroundColor`
- **Input**: `toCapitalize.json`.
- **Output**: `seeded_beans_with_color_name.txt`, `list_of_colors.txt`.
  'seeded_beans_with_color_name.txt' - is the finalize data ready to be seeded into the API with all 'Bean' properties.
  'list_of_colors.txt' - a supporting file - table that list all the hex color code that exist in the seeded data, include the color nickname.

## 'Recipe' class scraping process:

### 1. Data Collection

#### (`getRecipe.py`)

- **Purpose**: The purpose of this script is to automate the web scraping of a specific webpage (https://www.jellybelly.com/jelly-belly-bean-recipes), extract information about jelly bean recipes, and store the extracted data in a structured format.
- **Input**: The input for this script is the URL of the webpage (https://www.jellybelly.com/jelly-belly-bean-recipes) containing jelly bean recipes.
- **Outputs**: The main output of this script is a list (scrapedRecipe.txt) that contains information about jelly bean recipes, including their names and ingredients, and this list is printed to the console as the final output.

### 2. Data Formatting

#### (`scrapedRecipeFormat.txt`)

- **Purpose**: Modify the scraped data format (manually with VS Code keyboard shortcuts) to match an array of strings in preparation for seeding.
- **Input**: `scrapedRecipe.txt`
- **Outputs**: `scrapedRecipeFormat.txt`

#### (`prepRecipeToSeed.txt`)

- **Purpose**: Modify the array of strings data format to match the Recipe model in preparation for seeding.
- **Input**: `scrapedRecipeFormat.txt`
- **Outputs**: `RecipeReadyToSeed.txt`

## 'Recipe' class scraping process:

### 1. Data Collection and formatting

#### (`factsReadyToSeed.txt`)

- **Purpose**: Retrieve 100 facts about Jelly Belly beans using an AI prompt, and subsequently, prompt the AI to format the data to prepare it for feeding into the 'Fact' model.
- **Input**: Prompt: Generate 100 interesting and informative facts about Jelly Belly beans. Once you have the facts, please format the data in a way that it can be readily used for training the 'Fact' model (the prompt includes set of rules for formatting)
- **Outputs**: `factsReadyToSeed.txt`
